{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1351797,"sourceType":"datasetVersion","datasetId":786787}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"ef5483f7","cell_type":"code","source":"# Imports\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import transforms, datasets\nfrom torch.utils.data import DataLoader, random_split\nimport os\n\nFER_213_PATH = \"/kaggle/input/fer2013/train\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T23:12:55.762057Z","iopub.execute_input":"2025-08-06T23:12:55.762328Z","iopub.status.idle":"2025-08-06T23:12:55.766347Z","shell.execute_reply.started":"2025-08-06T23:12:55.762306Z","shell.execute_reply":"2025-08-06T23:12:55.765653Z"}},"outputs":[],"execution_count":3},{"id":"80cf3ce8","cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T23:12:57.790825Z","iopub.execute_input":"2025-08-06T23:12:57.791096Z","iopub.status.idle":"2025-08-06T23:12:57.852986Z","shell.execute_reply.started":"2025-08-06T23:12:57.791074Z","shell.execute_reply":"2025-08-06T23:12:57.852289Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":4},{"id":"3643ff0d","cell_type":"code","source":"# Getting data ready\ntranform = transforms.Compose([\n    transforms.Resize((48, 48)),\n    transforms.Grayscale(num_output_channels=3),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(20),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],  # ImageNet stats\n                         std=[0.229, 0.224, 0.225])\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T23:12:58.940855Z","iopub.execute_input":"2025-08-06T23:12:58.941123Z","iopub.status.idle":"2025-08-06T23:12:58.945975Z","shell.execute_reply.started":"2025-08-06T23:12:58.941102Z","shell.execute_reply":"2025-08-06T23:12:58.945374Z"}},"outputs":[],"execution_count":5},{"id":"65a9c757","cell_type":"code","source":"# Importing dataset\ndataset = datasets.ImageFolder(root=FER_213_PATH, transform=tranform)\n\ntrain_size = int(0.8 * len(dataset))\nval_size = len(dataset) - train_size\ntrain_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n\nprint(\"Classification Options:\", dataset.class_to_idx)\nprint(\"Number of training samples:\", len(train_dataset))\nprint(\"Number of validation samples:\", len(val_dataset))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T23:13:00.152825Z","iopub.execute_input":"2025-08-06T23:13:00.153119Z","iopub.status.idle":"2025-08-06T23:13:34.074771Z","shell.execute_reply.started":"2025-08-06T23:13:00.153097Z","shell.execute_reply":"2025-08-06T23:13:34.074143Z"}},"outputs":[{"name":"stdout","text":"Classification Options: {'angry': 0, 'disgust': 1, 'fear': 2, 'happy': 3, 'neutral': 4, 'sad': 5, 'surprise': 6}\nNumber of training samples: 22967\nNumber of validation samples: 5742\n","output_type":"stream"}],"execution_count":6},{"id":"d7dd9c6e","cell_type":"code","source":"# Load a pretrained RestNet18 model\nfrom torchvision.models import resnet18, ResNet18_Weights\nfrom torchsummary import summary\n\nweights = ResNet18_Weights.DEFAULT\nmodel = resnet18(weights=weights)\n\n# Modify the final layer for 7 classes\nmodel.fc = nn.Linear(model.fc.in_features, 7)\n\n# Transfer learning only on the last residual block and the final fully connected\nfor name, param, in model.named_parameters():\n    if \"layer4\" not in name and \"fc\" not in name:\n        param.requires_grad = False\n    else:\n        param.requires_grad = True\n\nmodel = model.to(device)\n\nsummary(model, input_size=(3, 224, 224))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T23:13:55.328144Z","iopub.execute_input":"2025-08-06T23:13:55.328874Z","iopub.status.idle":"2025-08-06T23:13:56.916203Z","shell.execute_reply.started":"2025-08-06T23:13:55.328844Z","shell.execute_reply":"2025-08-06T23:13:56.915400Z"}},"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n100%|██████████| 44.7M/44.7M [00:00<00:00, 185MB/s]\n","output_type":"stream"},{"name":"stdout","text":"----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1         [-1, 64, 112, 112]           9,408\n       BatchNorm2d-2         [-1, 64, 112, 112]             128\n              ReLU-3         [-1, 64, 112, 112]               0\n         MaxPool2d-4           [-1, 64, 56, 56]               0\n            Conv2d-5           [-1, 64, 56, 56]          36,864\n       BatchNorm2d-6           [-1, 64, 56, 56]             128\n              ReLU-7           [-1, 64, 56, 56]               0\n            Conv2d-8           [-1, 64, 56, 56]          36,864\n       BatchNorm2d-9           [-1, 64, 56, 56]             128\n             ReLU-10           [-1, 64, 56, 56]               0\n       BasicBlock-11           [-1, 64, 56, 56]               0\n           Conv2d-12           [-1, 64, 56, 56]          36,864\n      BatchNorm2d-13           [-1, 64, 56, 56]             128\n             ReLU-14           [-1, 64, 56, 56]               0\n           Conv2d-15           [-1, 64, 56, 56]          36,864\n      BatchNorm2d-16           [-1, 64, 56, 56]             128\n             ReLU-17           [-1, 64, 56, 56]               0\n       BasicBlock-18           [-1, 64, 56, 56]               0\n           Conv2d-19          [-1, 128, 28, 28]          73,728\n      BatchNorm2d-20          [-1, 128, 28, 28]             256\n             ReLU-21          [-1, 128, 28, 28]               0\n           Conv2d-22          [-1, 128, 28, 28]         147,456\n      BatchNorm2d-23          [-1, 128, 28, 28]             256\n           Conv2d-24          [-1, 128, 28, 28]           8,192\n      BatchNorm2d-25          [-1, 128, 28, 28]             256\n             ReLU-26          [-1, 128, 28, 28]               0\n       BasicBlock-27          [-1, 128, 28, 28]               0\n           Conv2d-28          [-1, 128, 28, 28]         147,456\n      BatchNorm2d-29          [-1, 128, 28, 28]             256\n             ReLU-30          [-1, 128, 28, 28]               0\n           Conv2d-31          [-1, 128, 28, 28]         147,456\n      BatchNorm2d-32          [-1, 128, 28, 28]             256\n             ReLU-33          [-1, 128, 28, 28]               0\n       BasicBlock-34          [-1, 128, 28, 28]               0\n           Conv2d-35          [-1, 256, 14, 14]         294,912\n      BatchNorm2d-36          [-1, 256, 14, 14]             512\n             ReLU-37          [-1, 256, 14, 14]               0\n           Conv2d-38          [-1, 256, 14, 14]         589,824\n      BatchNorm2d-39          [-1, 256, 14, 14]             512\n           Conv2d-40          [-1, 256, 14, 14]          32,768\n      BatchNorm2d-41          [-1, 256, 14, 14]             512\n             ReLU-42          [-1, 256, 14, 14]               0\n       BasicBlock-43          [-1, 256, 14, 14]               0\n           Conv2d-44          [-1, 256, 14, 14]         589,824\n      BatchNorm2d-45          [-1, 256, 14, 14]             512\n             ReLU-46          [-1, 256, 14, 14]               0\n           Conv2d-47          [-1, 256, 14, 14]         589,824\n      BatchNorm2d-48          [-1, 256, 14, 14]             512\n             ReLU-49          [-1, 256, 14, 14]               0\n       BasicBlock-50          [-1, 256, 14, 14]               0\n           Conv2d-51            [-1, 512, 7, 7]       1,179,648\n      BatchNorm2d-52            [-1, 512, 7, 7]           1,024\n             ReLU-53            [-1, 512, 7, 7]               0\n           Conv2d-54            [-1, 512, 7, 7]       2,359,296\n      BatchNorm2d-55            [-1, 512, 7, 7]           1,024\n           Conv2d-56            [-1, 512, 7, 7]         131,072\n      BatchNorm2d-57            [-1, 512, 7, 7]           1,024\n             ReLU-58            [-1, 512, 7, 7]               0\n       BasicBlock-59            [-1, 512, 7, 7]               0\n           Conv2d-60            [-1, 512, 7, 7]       2,359,296\n      BatchNorm2d-61            [-1, 512, 7, 7]           1,024\n             ReLU-62            [-1, 512, 7, 7]               0\n           Conv2d-63            [-1, 512, 7, 7]       2,359,296\n      BatchNorm2d-64            [-1, 512, 7, 7]           1,024\n             ReLU-65            [-1, 512, 7, 7]               0\n       BasicBlock-66            [-1, 512, 7, 7]               0\nAdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n           Linear-68                    [-1, 7]           3,591\n================================================================\nTotal params: 11,180,103\nTrainable params: 8,397,319\nNon-trainable params: 2,782,784\n----------------------------------------------------------------\nInput size (MB): 0.57\nForward/backward pass size (MB): 62.79\nParams size (MB): 42.65\nEstimated Total Size (MB): 106.01\n----------------------------------------------------------------\n","output_type":"stream"}],"execution_count":8},{"id":"2a3de09e","cell_type":"code","source":"loss_function = nn.CrossEntropyLoss()\noptimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T23:14:04.731600Z","iopub.execute_input":"2025-08-06T23:14:04.732449Z","iopub.status.idle":"2025-08-06T23:14:04.737016Z","shell.execute_reply.started":"2025-08-06T23:14:04.732414Z","shell.execute_reply":"2025-08-06T23:14:04.736303Z"}},"outputs":[],"execution_count":9},{"id":"b224b3f1","cell_type":"code","source":"# Training loop\nnum_epochs = 30\nbest_val_accuracy = 0.0\npatience = 5\nearly_stop_counter = 0\n\nfor epoch in range(num_epochs):\n    model.train()\n    total_loss = 0.0\n    correct = 0\n    total = 0\n\n    for images, labels in train_loader:\n        images, labels = images.to(device), labels.to(device)\n\n        outputs = model(images)\n        loss = loss_function(outputs, labels)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n        _, predicted = torch.max(outputs, 1)\n        correct += (predicted == labels).sum().item()\n        total += labels.size(0)\n\n    train_acc = 100 * correct / total\n\n    # Validation data\n    model.eval()\n    val_correct = 0\n    val_total = 0\n\n    with torch.no_grad():\n        for images, labels in val_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            _, predicted = torch.max(outputs, 1)\n            val_correct += (predicted == labels).sum().item()\n            val_total += labels.size(0)\n\n    val_acc = 100 * val_correct / val_total\n    scheduler.step(val_acc)\n    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(train_loader):.4f}, \"\n          f\"Train Accuracy: {train_acc:.2f}%, Val Accuracy: {val_acc:.2f}%\")\n    \n    # Early stopping\n    if val_acc > best_val_accuracy:\n        best_val_accuracy = val_acc\n        early_stop_counter = 0\n        torch.save(model.state_dict(), 'best_FER_2013_model.pth')\n    else:\n        early_stop_counter += 1\n        if early_stop_counter >= patience:\n            print(\"Early stopping triggered, training stopped.\")\n            break\n\n# Some verbose output\nprint(f\"Best validation accuracy: {best_val_accuracy:.2f}%\")\nprint(\"Training complete. Model saved as 'best_FER_2013_model.pth'.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T23:17:09.624571Z","iopub.execute_input":"2025-08-06T23:17:09.625104Z","iopub.status.idle":"2025-08-06T23:35:51.807193Z","shell.execute_reply.started":"2025-08-06T23:17:09.625071Z","shell.execute_reply":"2025-08-06T23:35:51.806509Z"}},"outputs":[{"name":"stdout","text":"Epoch [1/30], Loss: 1.5317, Train Accuracy: 40.84%, Val Accuracy: 41.55%\nEpoch [2/30], Loss: 1.4667, Train Accuracy: 43.35%, Val Accuracy: 42.96%\nEpoch [3/30], Loss: 1.4273, Train Accuracy: 44.90%, Val Accuracy: 45.89%\nEpoch [4/30], Loss: 1.3859, Train Accuracy: 47.13%, Val Accuracy: 44.95%\nEpoch [5/30], Loss: 1.3663, Train Accuracy: 47.96%, Val Accuracy: 44.95%\nEpoch [6/30], Loss: 1.3374, Train Accuracy: 49.20%, Val Accuracy: 46.45%\nEpoch [7/30], Loss: 1.3163, Train Accuracy: 49.63%, Val Accuracy: 46.59%\nEpoch [8/30], Loss: 1.2909, Train Accuracy: 50.99%, Val Accuracy: 46.45%\nEpoch [9/30], Loss: 1.2681, Train Accuracy: 51.89%, Val Accuracy: 47.58%\nEpoch [10/30], Loss: 1.2465, Train Accuracy: 52.45%, Val Accuracy: 47.37%\nEpoch [11/30], Loss: 1.2253, Train Accuracy: 53.65%, Val Accuracy: 47.47%\nEpoch [12/30], Loss: 1.2045, Train Accuracy: 54.59%, Val Accuracy: 48.64%\nEpoch [13/30], Loss: 1.1864, Train Accuracy: 55.06%, Val Accuracy: 49.20%\nEpoch [14/30], Loss: 1.1588, Train Accuracy: 56.19%, Val Accuracy: 47.98%\nEpoch [15/30], Loss: 1.1406, Train Accuracy: 57.18%, Val Accuracy: 48.62%\nEpoch [16/30], Loss: 1.1302, Train Accuracy: 57.72%, Val Accuracy: 48.87%\nEpoch [17/30], Loss: 1.1058, Train Accuracy: 58.52%, Val Accuracy: 49.18%\nEpoch [18/30], Loss: 1.0438, Train Accuracy: 61.41%, Val Accuracy: 49.20%\nEarly stopping triggered, training stopped.\nBest validation accuracy: 49.20%\nTraining complete. Model saved as 'best_FER_2013_model.pth'.\n","output_type":"stream"}],"execution_count":11}]}